{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxl_LWeYQ3pk",
        "outputId": "634d901f-b3f8-4a6f-c1b4-bd01f515f149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.9.0)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.9)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.72)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.3.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0.post1\n"
          ]
        }
      ],
      "source": [
        "#Install required packages\n",
        "!pip install langchain numpy python-dotenv rank-bm25\n",
        "!pip install pypdf\n",
        "!pip install PyMuPDF\n",
        "!pip install python-dotenv\n",
        "!pip install langchain-community\n",
        "!pip install sentence-transformers\n",
        "!pip install langchain-google-genai\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clone the repository to access helper functions and evaluation modules\n",
        "!git clone https://github.com/databytobi/RAG_TECHNIQUES.git\n",
        "import sys\n",
        "sys.path.append('RAG_TECHNIQUES')\n",
        "# If you need to run with the latest data\n",
        "# !cp -r RAG_TECHNIQUES/data ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0nuIQ_HSasX",
        "outputId": "c4853123-7e2a-41c8-8bca-4ec550c03217"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RAG_TECHNIQUES'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 68 (delta 19), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (68/68), 10.02 MiB | 15.98 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "from typing import List\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set the OpenAI API key environment variable (comment out if not using GOOGLE)\n",
        "if not os.getenv('GOOGLE_API_KEY'):\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = input(\"Please enter your GOOGLE API key: \")\n",
        "else:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "# Original path append replaced for Colab compatibility\n",
        "from helper_functions import *\n",
        "#from evaluation.evalute_rag import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCOWODjFR52Z",
        "outputId": "2155d094-167d-4179-b255-87c113d22448"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3122736334.py:20: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  from helper_functions import *\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define document path"
      ],
      "metadata": {
        "id": "DjVTcG09Sro9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Agents_v8.pdf https://raw.githubusercontent.com/databytobi/RAG_TECHNIQUES/main/data/Agents_v8.pdf\n",
        "!wget -O data/Agents_v8.pdf https://raw.githubusercontent.com/databytobi/RAG_TECHNIQUES/main/data/Agents_v8.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDe9h_QQSz4O",
        "outputId": "13abeec6-4e1c-4c2b-c65a-262bc33a32b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-05 12:55:00--  https://raw.githubusercontent.com/databytobi/RAG_TECHNIQUES/main/data/Agents_v8.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9305713 (8.9M) [application/octet-stream]\n",
            "Saving to: ‘data/Agents_v8.pdf’\n",
            "\n",
            "data/Agents_v8.pdf  100%[===================>]   8.87M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-08-05 12:55:00 (88.6 MB/s) - ‘data/Agents_v8.pdf’ saved [9305713/9305713]\n",
            "\n",
            "--2025-08-05 12:55:00--  https://raw.githubusercontent.com/databytobi/RAG_TECHNIQUES/main/data/Agents_v8.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9305713 (8.9M) [application/octet-stream]\n",
            "Saving to: ‘data/Agents_v8.pdf’\n",
            "\n",
            "data/Agents_v8.pdf  100%[===================>]   8.87M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-08-05 12:55:00 (89.0 MB/s) - ‘data/Agents_v8.pdf’ saved [9305713/9305713]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"data/Agents_v8.pdf\""
      ],
      "metadata": {
        "id": "2X6wRB-mTI50"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the pdf to vector store and return split document from the step before, to create BM25 instance"
      ],
      "metadata": {
        "id": "_NYWYRoqTXZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_pdf_and_get_split_documents(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = HuggingFaceEmbeddings()\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore, cleaned_texts"
      ],
      "metadata": {
        "id": "RcFm_E2pTa13"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create vectorstore and get the chunked documents"
      ],
      "metadata": {
        "id": "xPCBeMjqTlHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore, cleaned_texts = encode_pdf_and_get_split_documents(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFVOtkTFT1Zy",
        "outputId": "76a8b99d-3bcf-4179-9f14-0629b13a31d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1688076021.py:26: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embeddings = HuggingFaceEmbeddings()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a bm25 index for retrieving documents by keywords"
      ],
      "metadata": {
        "id": "5UUqi6BBUkyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bm25_index(documents: List[Document]) -> BM25Okapi:\n",
        "    \"\"\"\n",
        "    Create a BM25 index from the given documents.\n",
        "\n",
        "    BM25 (Best Matching 25) is a ranking function used in information retrieval.\n",
        "    It's based on the probabilistic retrieval framework and is an improvement over TF-IDF.\n",
        "\n",
        "    Args:\n",
        "    documents (List[Document]): List of documents to index.\n",
        "\n",
        "    Returns:\n",
        "    BM25Okapi: An index that can be used for BM25 scoring.\n",
        "    \"\"\"\n",
        "    # Tokenize each document by splitting on whitespace\n",
        "    # This is a simple approach and could be improved with more sophisticated tokenization\n",
        "    tokenized_docs = [doc.page_content.split() for doc in documents]\n",
        "    return BM25Okapi(tokenized_docs)"
      ],
      "metadata": {
        "id": "M5sdEEL7UIGn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25 = create_bm25_index(cleaned_texts) # Create BM25 index from the cleaned texts (chunks)"
      ],
      "metadata": {
        "id": "9tdNuW-MVpwK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function that retrieves both semantically and by keyword, normalizes the scores and gets the top k documents"
      ],
      "metadata": {
        "id": "xuV689xTV5Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fusion_retrieval(vectorstore, bm25, query: str, k: int = 5, alpha: float = 0.5) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Perform fusion retrieval combining keyword-based (BM25) and vector-based search.\n",
        "\n",
        "    Args:\n",
        "    vectorstore (VectorStore): The vectorstore containing the documents.\n",
        "    bm25 (BM25Okapi): Pre-computed BM25 index.\n",
        "    query (str): The query string.\n",
        "    k (int): The number of documents to retrieve.\n",
        "    alpha (float): The weight for vector search scores (1-alpha will be the weight for BM25 scores).\n",
        "\n",
        "    Returns:\n",
        "    List[Document]: The top k documents based on the combined scores.\n",
        "    \"\"\"\n",
        "\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # Step 1: Get all documents from the vectorstore\n",
        "    all_docs = vectorstore.similarity_search(\"\", k=vectorstore.index.ntotal)\n",
        "\n",
        "    # Step 2: Perform BM25 search\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "\n",
        "    # Step 3: Perform vector search\n",
        "    vector_results = vectorstore.similarity_search_with_score(query, k=len(all_docs))\n",
        "\n",
        "    # Step 4: Normalize scores\n",
        "    vector_scores = np.array([score for _, score in vector_results])\n",
        "    vector_scores = 1 - (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
        "\n",
        "    bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) -  np.min(bm25_scores) + epsilon)\n",
        "\n",
        "    # Step 5: Combine scores\n",
        "    combined_scores = alpha * vector_scores + (1 - alpha) * bm25_scores\n",
        "\n",
        "    # Step 6: Rank documents\n",
        "    sorted_indices = np.argsort(combined_scores)[::-1]\n",
        "\n",
        "    # Step 7: Return top k documents\n",
        "    return [all_docs[i] for i in sorted_indices[:k]]"
      ],
      "metadata": {
        "id": "NyIy1-o4VtM8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use case"
      ],
      "metadata": {
        "id": "z4w8bff2WKBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Query\n",
        "query = \"What an agent?\"\n",
        "\n",
        "# Perform fusion retrieval\n",
        "top_docs = fusion_retrieval(vectorstore, bm25, query, k=5, alpha=0.5)\n",
        "docs_content = [doc.page_content for doc in top_docs]\n",
        "show_context(docs_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J61q1rpuWE92",
        "outputId": "9920ec47-d4b5-438d-fee7-6bec0304fdd3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context 1:\n",
            "Agents\n",
            "37\n",
            "February 2025\n",
            "============================== Human Message ================================\n",
            "Who did the Texas Longhorns play in football last week? What is the address \n",
            "of the other team's stadium?\n",
            "================================= Ai Message =================================\n",
            "Tool Calls: search\n",
            "Args:\n",
            " query: Texas Longhorns football schedule\n",
            "================================ Tool Message ================================\n",
            "Name: search\n",
            "{...Results: \"NCAA Division I Football, Georgia, Date...\"}\n",
            "================================= Ai Message =================================\n",
            "The Texas Longhorns played the Georgia Bulldogs last week.\n",
            "Tool Calls: places\n",
            "Args:\n",
            " query: Georgia Bulldogs stadium\n",
            "================================ Tool Message ================================\n",
            "Name: places\n",
            "{...Sanford Stadium Address: 100 Sanford...}\n",
            "================================= Ai Message =================================\n",
            "The address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA\n",
            "\n",
            "\n",
            "Context 2:\n",
            "Agents\n",
            "2\n",
            "February 2025\n",
            "Acknowledgements\n",
            "Content contributors\n",
            "Evan Huang\n",
            "Emily Xue\n",
            "Olcan Sercinoglu\n",
            "Sebastian Riedel\n",
            "Satinder Baveja\n",
            "Antonio Gulli\n",
            "Anant Nawalgaria\n",
            "Curators and Editors\n",
            "Antonio Gulli\n",
            "Anant Nawalgaria\n",
            "Grace Mollison \n",
            "Technical Writer\n",
            "Joey Haymaker\n",
            "Designer\n",
            "Michael Lanning\n",
            "\n",
            "\n",
            "Context 3:\n",
            "Agents\n",
            "36\n",
            "February 2025\n",
            "Python\n",
            "from langgraph.prebuilt import create_react_agent\n",
            "from langchain_core.tools import tool\n",
            "from langchain_community.utilities import SerpAPIWrapper\n",
            "from langchain_community.tools import GooglePlacesTool\n",
            "os.environ[\"SERPAPI_API_KEY\"] = \"XXXXX\"\n",
            "os.environ[\"GPLACES_API_KEY\"] = \"XXXXX\"\n",
            "@tool\n",
            "def search(query: str):\n",
            " \"\"\"Use the SerpAPI to run a Google Search.\"\"\"\n",
            " search = SerpAPIWrapper()\n",
            " return  search.run(query)\n",
            "@tool\n",
            "def places(query: str):\n",
            " \"\"\"Use the Google Places API to run a Google Places Query.\"\"\"\n",
            " places = GooglePlacesTool()\n",
            " return  places.run(query)\n",
            "model = ChatVertexAI(model=\"gemini-2.0-flash-001\")\n",
            "tools = [search, places]\n",
            "query = \"Who did the Texas Longhorns play in football last week? What is the \n",
            "address of the other team's stadium?\"\n",
            "agent = create_react_agent(model, tools)\n",
            "input = {\"messages\": [(\"human\", query)]}\n",
            "for s in agent.stream(input, stream_mode=\"values\"):\n",
            " message = s[\"messages\"][-1]\n",
            " if isinstance (message, tuple):\n",
            "  print(message )\n",
            "\n",
            "\n",
            "Context 4:\n",
            "Agents\n",
            "25\n",
            "February 2025\n",
            "Python\n",
            "from typing import Optional\n",
            "def display_cities(cities: list[str], preferences: Optional[str] = None):\n",
            " \"\"\"Provides a list of cities based on the user's search query and preferences.\n",
            " Args:\n",
            "  preferences (str): The user's preferences for the search, like skiing,\n",
            "  beach, restaurants, bbq, etc.\n",
            "  cities (list[str]): The list of cities being recommended to the user.\n",
            " Returns:\n",
            "  list[str]: The list of cities being recommended to the user.\n",
            " \"\"\"\n",
            " return cities\n",
            " \n",
            "Snippet 6. Sample python method for a function that will display a list of cities.\n",
            "Next, we’ll instantiate our model, build the Tool, then pass in our user’s query and tools to \n",
            "the model. Executing the code below would result in the output as seen at the bottom of the \n",
            "code snippet.\n",
            "\n",
            "\n",
            "Context 5:\n",
            "Agents\n",
            "35\n",
            "February 2025\n",
            "Agent quick start with LangChain\n",
            "In order to provide a real-world executable example of an agent in action, we’ll build a quick \n",
            "prototype with the LangChain and LangGraph libraries. These popular open source libraries \n",
            "allow users to build customer agents by “chaining” together sequences of logic, reasoning, \n",
            "and tool calls to answer a user’s query. We’ll use our gemini-2.0-flash-001 model and \n",
            "some simple tools to answer a multi-stage query from the user as seen in Snippet 8.\n",
            "The tools we are using are the SerpAPI (for Google Search) and the Google Places API. After \n",
            "executing our program in Snippet 8, you can see the sample output in Snippet 9.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Query\n",
        "query = \"usin Cognitive architectures: can you explain How agents operate ?\"\n",
        "\n",
        "# Perform fusion retrieval\n",
        "top_docs = fusion_retrieval(vectorstore, bm25, query, k=5, alpha=0.5)\n",
        "docs_content = [doc.page_content for doc in top_docs]\n",
        "show_context(docs_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow1pqSooWUOO",
        "outputId": "8d80381b-33fb-4aac-bbd2-a001779b1313"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context 1:\n",
            "Agents\n",
            "37\n",
            "February 2025\n",
            "============================== Human Message ================================\n",
            "Who did the Texas Longhorns play in football last week? What is the address \n",
            "of the other team's stadium?\n",
            "================================= Ai Message =================================\n",
            "Tool Calls: search\n",
            "Args:\n",
            " query: Texas Longhorns football schedule\n",
            "================================ Tool Message ================================\n",
            "Name: search\n",
            "{...Results: \"NCAA Division I Football, Georgia, Date...\"}\n",
            "================================= Ai Message =================================\n",
            "The Texas Longhorns played the Georgia Bulldogs last week.\n",
            "Tool Calls: places\n",
            "Args:\n",
            " query: Georgia Bulldogs stadium\n",
            "================================ Tool Message ================================\n",
            "Name: places\n",
            "{...Sanford Stadium Address: 100 Sanford...}\n",
            "================================= Ai Message =================================\n",
            "The address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA\n",
            "\n",
            "\n",
            "Context 2:\n",
            "Agents\n",
            "9\n",
            "February 2025\n",
            "• They gather information, like the patron’s order and what ingredients are in the pantry \n",
            "and refrigerator.\n",
            "• They perform some internal reasoning about what dishes and flavor profiles they can \n",
            "create based on the information they have just gathered.\n",
            "• They take action to create the dish: chopping vegetables, blending spices, searing meat.\n",
            "At each stage in the process the chef makes adjustments as needed, refining their plan as \n",
            "ingredients are depleted or customer feedback is received, and uses the set of previous \n",
            "outcomes to determine the next plan of action. This cycle of information intake, planning, \n",
            "executing, and adjusting describes a unique cognitive architecture that the chef employs to \n",
            "reach their goal.\n",
            "Just like the chef, agents can use cognitive architectures to reach their end goals by \n",
            "iteratively processing information, making informed decisions, and refining next actions\n",
            "\n",
            "\n",
            "Context 3:\n",
            "Agents\n",
            "36\n",
            "February 2025\n",
            "Python\n",
            "from langgraph.prebuilt import create_react_agent\n",
            "from langchain_core.tools import tool\n",
            "from langchain_community.utilities import SerpAPIWrapper\n",
            "from langchain_community.tools import GooglePlacesTool\n",
            "os.environ[\"SERPAPI_API_KEY\"] = \"XXXXX\"\n",
            "os.environ[\"GPLACES_API_KEY\"] = \"XXXXX\"\n",
            "@tool\n",
            "def search(query: str):\n",
            " \"\"\"Use the SerpAPI to run a Google Search.\"\"\"\n",
            " search = SerpAPIWrapper()\n",
            " return  search.run(query)\n",
            "@tool\n",
            "def places(query: str):\n",
            " \"\"\"Use the Google Places API to run a Google Places Query.\"\"\"\n",
            " places = GooglePlacesTool()\n",
            " return  places.run(query)\n",
            "model = ChatVertexAI(model=\"gemini-2.0-flash-001\")\n",
            "tools = [search, places]\n",
            "query = \"Who did the Texas Longhorns play in football last week? What is the \n",
            "address of the other team's stadium?\"\n",
            "agent = create_react_agent(model, tools)\n",
            "input = {\"messages\": [(\"human\", query)]}\n",
            "for s in agent.stream(input, stream_mode=\"values\"):\n",
            " message = s[\"messages\"][-1]\n",
            " if isinstance (message, tuple):\n",
            "  print(message )\n",
            "\n",
            "\n",
            "Context 4:\n",
            "Agents\n",
            "22\n",
            "February 2025\n",
            "function_call {\n",
            "  name: \"display_cities\"\n",
            "  args: {\n",
            "    \"cities\": [\"Crested Butte\", \"Whistler\", \"Zermatt\"],\n",
            "    \"preferences\": \"skiing\"\n",
            "    }\n",
            "}\n",
            "Snippet 5. Sample Function Call payload for displaying a list of cities and user preferences\n",
            "This JSON payload is generated by the model, and then sent to our Client-side server to do \n",
            "whatever we would like to do with it. In this specific case, we’ll call the Google Places API to \n",
            "take the cities provided by the model and look up Images, then provide them as formatted \n",
            "rich content back to our User. Consider this sequence diagram in Figure 9 showing the above \n",
            "interaction in step by step detail.\n",
            "\n",
            "\n",
            "Context 5:\n",
            "Agents\n",
            "2\n",
            "February 2025\n",
            "Acknowledgements\n",
            "Content contributors\n",
            "Evan Huang\n",
            "Emily Xue\n",
            "Olcan Sercinoglu\n",
            "Sebastian Riedel\n",
            "Satinder Baveja\n",
            "Antonio Gulli\n",
            "Anant Nawalgaria\n",
            "Curators and Editors\n",
            "Antonio Gulli\n",
            "Anant Nawalgaria\n",
            "Grace Mollison \n",
            "Technical Writer\n",
            "Joey Haymaker\n",
            "Designer\n",
            "Michael Lanning\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#Query\n",
        "query = \"can you explain how to Enhancing model performance with targeted learning?\"\n",
        "\n",
        "\n",
        "# Perform fusion retrieval\n",
        "top_docs = fusion_retrieval(vectorstore, bm25, query, k=5, alpha=0.5)\n",
        "docs_content = [doc.page_content for doc in top_docs]\n",
        "show_context(docs_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtI6HnQoW5uJ",
        "outputId": "b0dd2fed-afe1-484a-eed5-3ad577c9f276"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context 1:\n",
            "Agents\n",
            "37\n",
            "February 2025\n",
            "============================== Human Message ================================\n",
            "Who did the Texas Longhorns play in football last week? What is the address \n",
            "of the other team's stadium?\n",
            "================================= Ai Message =================================\n",
            "Tool Calls: search\n",
            "Args:\n",
            " query: Texas Longhorns football schedule\n",
            "================================ Tool Message ================================\n",
            "Name: search\n",
            "{...Results: \"NCAA Division I Football, Georgia, Date...\"}\n",
            "================================= Ai Message =================================\n",
            "The Texas Longhorns played the Georgia Bulldogs last week.\n",
            "Tool Calls: places\n",
            "Args:\n",
            " query: Georgia Bulldogs stadium\n",
            "================================ Tool Message ================================\n",
            "Name: places\n",
            "{...Sanford Stadium Address: 100 Sanford...}\n",
            "================================= Ai Message =================================\n",
            "The address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA\n",
            "\n",
            "\n",
            "Context 2:\n",
            "using a matching algorithm like SCaNN\n",
            "3. The matched content is retrieved from the vector database in text format and sent back to \n",
            "the agent\n",
            "4. The agent receives both the user query and retrieved content, then formulates a response \n",
            "or action\n",
            "\n",
            "\n",
            "Context 3:\n",
            "Agents\n",
            "22\n",
            "February 2025\n",
            "function_call {\n",
            "  name: \"display_cities\"\n",
            "  args: {\n",
            "    \"cities\": [\"Crested Butte\", \"Whistler\", \"Zermatt\"],\n",
            "    \"preferences\": \"skiing\"\n",
            "    }\n",
            "}\n",
            "Snippet 5. Sample Function Call payload for displaying a list of cities and user preferences\n",
            "This JSON payload is generated by the model, and then sent to our Client-side server to do \n",
            "whatever we would like to do with it. In this specific case, we’ll call the Google Places API to \n",
            "take the cities provided by the model and look up Images, then provide them as formatted \n",
            "rich content back to our User. Consider this sequence diagram in Figure 9 showing the above \n",
            "interaction in step by step detail.\n",
            "\n",
            "\n",
            "Context 4:\n",
            "choices. Ultimately, it is up to the application developer to choose what is right for the \n",
            "specific application.\n",
            "Function sample code\n",
            "To achieve the above output from our ski vacation scenario, let’s build out each of the \n",
            "components to make this work with our gemini-2.0-flash-001 model. \n",
            "First, we’ll define our display_cities function as a simple Python method.\n",
            "\n",
            "\n",
            "Context 5:\n",
            "Gemini “Show me flights from Austin to Zurich leaving next Friday.”\n",
            "Sample Extensions \n",
            "To simplify the usage of Extensions, Google provides some out of the box extensions that \n",
            "can be quickly imported into your project and used with minimal configurations. For example, \n",
            "the Code Interpreter extension in Snippet 1 allows you to generate and run Python code from \n",
            "a natural language description.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xvnKkSKdX8Y4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}