{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "raYoWKtD8BxG",
        "outputId": "163dbd2b-936b-44e4-f15d-8e8b9b28e8c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.8-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, python-dotenv, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.8 python-dotenv-1.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "e5d7c4a773a24392b593b898134936c8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Install required packages\n",
        "!pip install langchain langchain-google-genai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set the OpenAI API key environment variable\n",
        "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "3mj4kR7N-AoE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Query Rewriting: Reformulating queries to improve retrieval."
      ],
      "metadata": {
        "id": "Ts9Pq-08ADRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re_write_llm = ChatGoogleGenerativeAI (temperature=0, model=\"gemini-2.0-flash\", max_tokens=4000)\n",
        "\n",
        "# Create a prompt template for query rewriting\n",
        "query_rewrite_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system.\n",
        "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
        "\n",
        "Original query: {original_query}\n",
        "\n",
        "Rewritten query:\"\"\"\n",
        "\n",
        "query_rewrite_prompt = PromptTemplate(\n",
        "    input_variables=[\"original_query\"],\n",
        "    template=query_rewrite_template\n",
        ")\n",
        "\n",
        "# Create an LLMChain for query rewriting\n",
        "query_rewriter = query_rewrite_prompt | re_write_llm\n",
        "\n",
        "def rewrite_query(original_query):\n",
        "    \"\"\"\n",
        "    Rewrite the original query to improve retrieval.\n",
        "\n",
        "    Args:\n",
        "    original_query (str): The original user query\n",
        "\n",
        "    Returns:\n",
        "    str: The rewritten query\n",
        "    \"\"\"\n",
        "    response = query_rewriter.invoke(original_query)\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "vvGKZkCk_UhR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "implement on a use case"
      ],
      "metadata": {
        "id": "TYYy0uk-A1-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example query over the understanding climate change dataset\n",
        "original_query = \"What are the impacts of climate change on the environment?\"\n",
        "rewritten_query = rewrite_query(original_query)\n",
        "print(\"Original query:\", original_query)\n",
        "print(\"\\nRewritten query:\", rewritten_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEtEM2zPAbSz",
        "outputId": "5b7af389-b418-47eb-e9f5-1899e03e3401"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original query: What are the impacts of climate change on the environment?\n",
            "\n",
            "Rewritten query: What are the specific environmental impacts of anthropogenic climate change, including but not limited to effects on biodiversity, sea levels, ocean acidification, extreme weather events (such as hurricanes, droughts, and floods), and changes in ecosystem distribution and function? Please provide examples and cite relevant scientific studies or reports.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Step-back Prompting: Generating broader queries for better context retrieval."
      ],
      "metadata": {
        "id": "o-Qb_lIKBUTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_back_llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.0-flash\", max_tokens=4000)\n",
        "\n",
        "\n",
        "# Create a prompt template for step-back prompting\n",
        "step_back_template = \"\"\"You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
        "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
        "\n",
        "Original query: {original_query}\n",
        "\n",
        "Step-back query:\"\"\"\n",
        "\n",
        "step_back_prompt = PromptTemplate(\n",
        "    input_variables=[\"original_query\"],\n",
        "    template=step_back_template\n",
        ")\n",
        "\n",
        "# Create an LLMChain for step-back prompting\n",
        "step_back_chain = step_back_prompt | step_back_llm\n",
        "\n",
        "def generate_step_back_query(original_query):\n",
        "    \"\"\"\n",
        "    Generate a step-back query to retrieve broader context.\n",
        "\n",
        "    Args:\n",
        "    original_query (str): The original user query\n",
        "\n",
        "    Returns:\n",
        "    str: The step-back query\n",
        "    \"\"\"\n",
        "    response = step_back_chain.invoke(original_query)\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "en3FLdhdBHs2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "implement on a use case"
      ],
      "metadata": {
        "id": "DtXi8bZfCEjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example query over the understanding climate change dataset\n",
        "original_query = \"What are the impacts of climate change on the environment?\"\n",
        "step_back_query = generate_step_back_query(original_query)\n",
        "print(\"Original query:\", original_query)\n",
        "print(\"\\nStep-back query:\", step_back_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-jEL66hB5UP",
        "outputId": "18597cc1-647c-4787-f0eb-bf999c967af2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original query: What are the impacts of climate change on the environment?\n",
            "\n",
            "Step-back query: What is climate change?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Sub-query Decomposition: Breaking complex queries into simpler sub-queries."
      ],
      "metadata": {
        "id": "5dWvP_mUCSWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_query_llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.0-flash\", max_tokens=4000)\n",
        "\n",
        "# Create a prompt template for sub-query decomposition\n",
        "subquery_decomposition_template = \"\"\"You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
        "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
        "\n",
        "Original query: {original_query}\n",
        "\n",
        "example: What are the impacts of climate change on the environment?\n",
        "\n",
        "Sub-queries:\n",
        "1. What are the impacts of climate change on biodiversity?\n",
        "2. How does climate change affect the oceans?\n",
        "3. What are the effects of climate change on agriculture?\n",
        "4. What are the impacts of climate change on human health?\"\"\"\n",
        "\n",
        "\n",
        "subquery_decomposition_prompt = PromptTemplate(\n",
        "    input_variables=[\"original_query\"],\n",
        "    template=subquery_decomposition_template\n",
        ")\n",
        "\n",
        "# Create an LLMChain for sub-query decomposition\n",
        "subquery_decomposer_chain = subquery_decomposition_prompt | sub_query_llm\n",
        "\n",
        "def decompose_query(original_query: str):\n",
        "    \"\"\"\n",
        "    Decompose the original query into simpler sub-queries.\n",
        "\n",
        "    Args:\n",
        "    original_query (str): The original complex query\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of simpler sub-queries\n",
        "    \"\"\"\n",
        "    response = subquery_decomposer_chain.invoke(original_query).content\n",
        "    sub_queries = [q.strip() for q in response.split('\\n') if q.strip() and not q.strip().startswith('Sub-queries:')]\n",
        "    return sub_queries"
      ],
      "metadata": {
        "id": "Y2TsGDVYCJ38"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "implement on a use case"
      ],
      "metadata": {
        "id": "UT5V5vEmC18Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example query over the understanding climate change dataset\n",
        "original_query = \"What are the impacts of climate change on the environment?\"\n",
        "sub_queries = decompose_query(original_query)\n",
        "print(\"\\nSub-queries:\")\n",
        "for i, sub_query in enumerate(sub_queries, 1):\n",
        "    print(sub_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYCJY5POCwFE",
        "outputId": "d2798cd7-2f7f-4a88-ce78-7b1fd073b297"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sub-queries:\n",
            "Okay, I understand. Here's a breakdown of the query \"What are the impacts of climate change on the environment?\" into simpler sub-queries:\n",
            "**Sub-queries:**\n",
            "1.  How does climate change affect global temperatures and weather patterns?\n",
            "2.  What are the effects of climate change on water resources (e.g., glaciers, rivers, rainfall)?\n",
            "3.  How does climate change impact ecosystems and natural habitats?\n",
            "4.  What are the effects of climate change on air quality and atmospheric composition?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Sentiment analysis can be a valuable addition to a RAG system's query transformation pipeline by providing insights into the user's emotional state or attitude towards the query topic. This information can be leveraged to improve the relevance and tone of the retrieved information.\n",
        "\n",
        "**1. Benefits of Analyzing Query Sentiment:**\n",
        "\n",
        "Analyzing the sentiment of a user's query offers several benefits:\n",
        "\n",
        "*   **Improved Relevance:** Understanding the sentiment can help the system prioritize documents or passages that align with the user's emotional context. A negative query might benefit from retrieval of troubleshooting guides or explanations of issues, while a positive query might benefit from user testimonials or success stories.\n",
        "*   **Tailored Responses:** The RAG system can tailor the retrieved information and the final generated response to match the user's sentiment. For instance, a query with negative sentiment might warrant a more empathetic and problem-solution oriented response.\n",
        "*   **Enhanced User Experience:** By acknowledging and responding to the user's underlying sentiment, the RAG system can provide a more personalized and helpful interaction, leading to a better user experience.\n",
        "*   **Identifying Urgency or Severity:** Extreme negative sentiment might indicate a critical issue or an urgent need for specific information, allowing the system to prioritize retrieval of crucial documents.\n",
        "\n",
        "**2. Specific Uses of Sentiment Information in Retrieval:**\n",
        "\n",
        "Sentiment information can influence the retrieval process in several ways:\n",
        "\n",
        "*   **Filtering or Ranking Search Results:** Retrieved documents can be filtered or ranked based on their own sentiment score relative to the query's sentiment. For a negative query about a product, the system might prioritize retrieving reviews or forum discussions that also express negative sentiment, as these might contain relevant problem descriptions or warnings. Conversely, for a positive query, it might prioritize positive reviews or marketing materials.\n",
        "*   **Selecting Knowledge Bases or Document Subsets:** Depending on the sentiment, the system could be directed to search within specific subsets of the knowledge base. A negative query about a product might trigger a search primarily within support forums or bug reports, while a positive query might focus on product features or success stories.\n",
        "*   **Adjusting Retrieval Parameters:** Sentiment could influence retrieval parameters like the desired level of detail or the type of information sought. A query with high frustration might lead the system to look for concise, direct answers and troubleshooting steps.\n",
        "*   **Influencing Reranking:** After initial retrieval, a reranking step could heavily weigh documents that match the query's sentiment, bringing the most emotionally relevant information to the top.\n",
        "\n",
        "**Example:** If a user queries \"This product is terrible, why does it keep crashing?\", the highly negative sentiment could trigger the retrieval system to:\n",
        "\n",
        "*   Prioritize documents from support forums or troubleshooting guides related to product crashes.\n",
        "*   Filter out overly positive marketing materials.\n",
        "*   Look for specific error codes or common issues reported by other users.\n",
        "*   Rank user reviews that also mention crashing issues higher.\n",
        "\n",
        "**3. Potential Challenges and Limitations:**\n",
        "\n",
        "Using sentiment analysis in RAG systems also presents challenges:\n",
        "\n",
        "*   **Accuracy of Sentiment Analysis:** Sentiment analysis can be complex, especially with nuanced language, sarcasm, or domain-specific jargon. Inaccurate sentiment detection can lead to suboptimal retrieval.\n",
        "*   **Context Dependency:** The sentiment of a query can be highly context-dependent. A negative word might not indicate negative sentiment in all situations (e.g., \"critically acclaimed\").\n",
        "*   **Ambiguous Queries:** Some queries may have neutral or mixed sentiment, making it difficult to use sentiment as a strong retrieval signal.\n",
        "*   **Data Requirements:** Training or fine-tuning sentiment models for specific domains or query types might require labeled data.\n",
        "*   **Integration Complexity:** Integrating sentiment analysis seamlessly into the existing RAG pipeline requires careful design and implementation.\n",
        "*   **Potential for Bias:** The sentiment analysis model itself might have biases that could affect retrieval fairness or accuracy."
      ],
      "metadata": {
        "id": "RILxnhTKHIXq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "431a8057"
      },
      "source": [
        "## Implement sentiment analysis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47379a92"
      },
      "source": [
        "\n",
        "Import the necessary library for sentiment analysis and define a function to perform sentiment analysis on a given query. NLTK is a suitable choice for this task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b57757ff",
        "outputId": "85357a1c-004e-4536-86f6-ff23eb700e51"
      },
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download VADER lexicon for sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "def analyze_sentiment(query: str):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment of a given query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user query.\n",
        "\n",
        "    Returns:\n",
        "        str: A sentiment category (positive, negative, neutral).\n",
        "    \"\"\"\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = analyzer.polarity_scores(query)\n",
        "\n",
        "    # Determine the sentiment category based on compound score\n",
        "    if sentiment_scores['compound'] >= 0.05:\n",
        "        return 'positive'\n",
        "    elif sentiment_scores['compound'] <= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d22e82e"
      },
      "source": [
        "\n",
        "Test the `analyze_sentiment` function with a few example queries to ensure it works as expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3cME1daE__f",
        "outputId": "06679cf4-d4da-494f-8eb5-18f49634c649"
      },
      "source": [
        "# Test the sentiment analysis function\n",
        "query1 = \"I love this product, it's amazing!\"\n",
        "query2 = \"This is the worst service I have ever received.\"\n",
        "query3 = \"The weather is neutral today.\"\n",
        "query4 = \"I am not happy with the results.\"\n",
        "\n",
        "sentiment1 = analyze_sentiment(query1)\n",
        "sentiment2 = analyze_sentiment(query2)\n",
        "sentiment3 = analyze_sentiment(query3)\n",
        "sentiment4 = analyze_sentiment(query4)\n",
        "\n",
        "print(f\"Query: '{query1}'\\nSentiment: {sentiment1}\\n\")\n",
        "print(f\"Query: '{query2}'\\nSentiment: {sentiment2}\\n\")\n",
        "print(f\"Query: '{query3}'\\nSentiment: {sentiment3}\\n\")\n",
        "print(f\"Query: '{query4}'\\nSentiment: {sentiment4}\\n\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: 'I love this product, it's amazing!'\n",
            "Sentiment: positive\n",
            "\n",
            "Query: 'This is the worst service I have ever received.'\n",
            "Sentiment: negative\n",
            "\n",
            "Query: 'The weather is neutral today.'\n",
            "Sentiment: neutral\n",
            "\n",
            "Query: 'I am not happy with the results.'\n",
            "Sentiment: negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acf56633"
      },
      "source": [
        "## Define query routing for rag\n",
        "\n",
        "\n",
        "lets Determine how query routing will be used to direct queries to the most appropriate retrieval method or data source.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing RAG Retrieval with Query Routing\n",
        "\n",
        "Query routing is a critical component in advanced RAG systems that allows the system to intelligently direct incoming user queries to the most appropriate retrieval method or data source. Instead of searching across a monolithic knowledge base with a single strategy, routing enables a more nuanced and efficient approach to information retrieval.\n",
        "\n",
        "**1. Benefits of Query Routing:**\n",
        "\n",
        "Implementing query routing offers several significant advantages:\n",
        "\n",
        "*   **Improved Relevance:** By directing queries to specialized knowledge bases or retrieval methods best suited for the query's nature, the system can retrieve more accurate and relevant information. For example, a query about a specific product feature might be routed to a product documentation knowledge base, while a query about general industry trends might be routed to a collection of news articles.\n",
        "*   **Increased Efficiency:** Routing can significantly reduce the search space, leading to faster retrieval times and lower computational costs. Instead of searching a vast, heterogeneous dataset, the system focuses on a smaller, more relevant subset.\n",
        "*   **Enhanced Scalability:** As the RAG system grows and incorporates more diverse data sources and retrieval techniques, routing provides a structured way to manage this complexity and ensure efficient operation.\n",
        "*   **Flexibility and Adaptability:** Query routing allows for easy integration of new data sources or retrieval methods without requiring a complete overhaul of the system. New routes can be added to handle specific types of queries or access new information.\n",
        "*   **Optimized Resource Utilization:** Different retrieval methods have varying computational requirements. Routing can direct queries to the most resource-efficient method that is still likely to provide a good answer.\n",
        "\n",
        "**2. Specific Criteria for Routing:**\n",
        "\n",
        "Query routing can be based on various criteria extracted from the user query:\n",
        "\n",
        "*   **Query Type:** Classifying queries based on their intent (e.g., factual question, comparison, instructional query, troubleshooting). A factual question might be routed to a structured knowledge base, while an instructional query might be directed to a collection of tutorials or guides.\n",
        "*   **Domain or Topic:** Identifying the subject matter of the query (e.g., finance, healthcare, technology, specific product). Queries related to a specific domain can be routed to specialized knowledge bases for that domain.\n",
        "*   **User Intent:** Understanding the underlying goal of the user (e.g., seeking information, solving a problem, making a decision). This can help route the query to resources that align with that intent.\n",
        "*   **Sentiment:** As discussed previously, the sentiment of the query can also be a routing criterion. Negative sentiment might route to troubleshooting guides, while positive sentiment might route to testimonials or success stories.\n",
        "*   **Keywords or Entities:** The presence of specific keywords, named entities (like product names or company names), or technical terms can be used to route queries to relevant data sources.\n",
        "*   **Query Complexity:** Simple queries might be handled by a basic retrieval method, while complex queries requiring multi-hop reasoning or synthesis might be routed to more sophisticated techniques or multiple data sources.\n",
        "\n",
        "**3. Different Routing Strategies:**\n",
        "\n",
        "Various strategies can be employed for query routing:\n",
        "\n",
        "*   **Rule-Based Routing:** This involves defining a set of explicit rules based on the query criteria. For example, a rule might state: \"If the query contains 'product X' and 'troubleshooting', route to the product X support documentation.\" This is straightforward to implement but can become complex to manage with a large number of rules.\n",
        "*   **Learned Routing (e.g., using Machine Learning Classifiers):** A machine learning model (like a classifier) can be trained to predict the best route based on features extracted from the query. This can be more flexible and scalable than rule-based systems, especially for handling nuanced queries. The model learns the optimal routing strategy from data.\n",
        "*   **Hybrid Approaches:** Combining rule-based and learned strategies can leverage the strengths of both. Simple, clear-cut cases can be handled by rules, while more ambiguous queries can be routed by a learned model.\n",
        "*   **LLM-Based Routing:** A large language model (LLM) can be used to analyze the query and determine the most appropriate route. The LLM can understand the query's context and intent more deeply than simpler methods.\n",
        "\n",
        "**4. Potential Challenges and Considerations:**\n",
        "\n",
        "Implementing effective query routing requires careful consideration of several challenges:\n",
        "\n",
        "*   **Defining Routing Criteria:** Identifying the most effective criteria for routing and how to accurately extract them from queries can be challenging.\n",
        "*   **Maintaining and Updating Routing Logic:** As the knowledge base and retrieval methods evolve, the routing logic needs to be updated accordingly.\n",
        "*   **Handling Ambiguous Queries:** Queries that don't clearly fit into a specific category can be difficult to route effectively.\n",
        "*   **Performance Overhead:** The routing process itself adds some overhead to the query processing pipeline. It's important to ensure that the routing logic is efficient.\n",
        "*   **Evaluating Routing Effectiveness:** Determining how well the routing strategy is performing requires metrics to assess whether queries are being sent to the most appropriate destinations.\n",
        "*   **Data Source Interoperability:** Ensuring that the retrieved information from different data sources can be seamlessly integrated and synthesized for the final response.\n",
        "\n",
        "By carefully designing and implementing a query routing mechanism, a RAG system can significantly improve its performance, efficiency, and ability to provide relevant and timely information to users.\n"
      ],
      "metadata": {
        "id": "febOdf1uH35n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce5c45b0"
      },
      "source": [
        "## Implement query routing\n",
        "\n",
        "\n",
        " implement the query routing logic based on the query's characteristics or sentiment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88e85fbd"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `route_query` function with conditional logic based on sentiment and test it with example queries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "fbea3161",
        "outputId": "b40ffb8e-88e0-4507-faf2-8de585b56ce9"
      },
      "source": [
        "def route_query(query: str, sentiment: str):\n",
        "    \"\"\"\n",
        "    Routes a query based on its sentiment.\n",
        "\n",
        "    Args:\n",
        "        query (str): The original user query.\n",
        "        sentiment (str): The sentiment of the query ('positive', 'negative', 'neutral').\n",
        "\n",
        "    Returns:\n",
        "        str: The determined route for the query.\n",
        "    \"\"\"\n",
        "    print(f\"Original query: '{query}'\")\n",
        "    print(f\"Query sentiment: {sentiment}\")\n",
        "\n",
        "    if sentiment == 'positive':\n",
        "        route = \"Route: Positive sentiment - Directing to success stories and testimonials.\"\n",
        "    elif sentiment == 'negative':\n",
        "        route = \"Route: Negative sentiment - Directing to troubleshooting guides and support resources.\"\n",
        "    else:  # Assuming 'neutral' or any other sentiment\n",
        "        route = \"Route: Neutral sentiment - Directing to general information.\"\n",
        "\n",
        "    print(route)\n",
        "    return route\n",
        "\n",
        "# Test cases\n",
        "route_query(\"I love this product, it's amazing!\", \"positive\")\n",
        "route_query(\"This is the worst service I have ever received.\", \"negative\")\n",
        "route_query(\"The weather is neutral today.\", \"neutral\")\n",
        "route_query(\"I am not happy with the results.\", \"negative\") # Testing a negative sentiment"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original query: 'I love this product, it's amazing!'\n",
            "Query sentiment: positive\n",
            "Route: Positive sentiment - Directing to success stories and testimonials.\n",
            "Original query: 'This is the worst service I have ever received.'\n",
            "Query sentiment: negative\n",
            "Route: Negative sentiment - Directing to troubleshooting guides and support resources.\n",
            "Original query: 'The weather is neutral today.'\n",
            "Query sentiment: neutral\n",
            "Route: Neutral sentiment - Directing to general information.\n",
            "Original query: 'I am not happy with the results.'\n",
            "Query sentiment: negative\n",
            "Route: Negative sentiment - Directing to troubleshooting guides and support resources.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Route: Negative sentiment - Directing to troubleshooting guides and support resources.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e793e3d"
      },
      "source": [
        "## Integrate techniques\n",
        "\n",
        "\n",
        " how to integrate all five techniques (Query Rewriting, Step-back Prompting, Sub-query Decomposition, Sentiment Analysis, and Query Routing) into a cohesive RAG workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrated RAG Workflow with Query Transformation Techniques\n",
        "\n",
        "This section outlines a possible workflow for a Retrieval Augmented Generation (RAG) system that integrates the five query transformation techniques: Query Rewriting, Step-back Prompting, Sub-query Decomposition, Sentiment Analysis, and Query Routing. The goal is to leverage these techniques to improve the relevance and effectiveness of the information retrieval and response generation process.\n",
        "\n",
        "**1. Workflow Overview**\n",
        "\n",
        "The proposed workflow involves a sequence of steps where the original user query is analyzed and transformed in multiple ways before engaging with the retrieval and generation components of the RAG system. The order of the steps is designed to build a richer understanding of the user's intent and information needs.\n",
        "\n",
        "Here is a possible order and explanation of the workflow:\n",
        "\n",
        "*   **Step 1: Receive Original Query:** The process begins when the RAG system receives the user's initial query.\n",
        "\n",
        "*   **Step 2: Sentiment Analysis:** The first transformation applied is Sentiment Analysis.\n",
        "    *   **Purpose:** To understand the emotional tone of the user's query. This provides early context about the user's state and can inform subsequent routing and response generation.\n",
        "    *   **Output:** The sentiment of the query (e.g., 'positive', 'negative', 'neutral').\n",
        "\n",
        "*   **Step 3: Query Routing:** Based on the sentiment and potentially other initial query characteristics (like keywords or a preliminary query type classification), the system performs Query Routing.\n",
        "    *   **Purpose:** To direct the query processing down the most appropriate path or towards the most relevant initial data sources. This allows for specialized handling of queries based on sentiment or domain.\n",
        "    *   **Output:** A determined 'route' or strategy for subsequent steps (e.g., prioritizing certain knowledge bases for negative sentiment, focusing on product features for positive sentiment). This step might influence which of the other transformation techniques are emphasized or how their outputs are utilized.\n",
        "\n",
        "*   **Step 4: Query Rewriting:** The original query is then passed through the Query Rewriting process.\n",
        "    *   **Purpose:** To create a more specific, detailed, and optimized version of the query for direct retrieval. This rewritten query aims to improve the precision of the initial search.\n",
        "    *   **Output:** A reformulated version of the original query.\n",
        "\n",
        "*   **Step 5: Step-back Prompting:** Simultaneously or in parallel with Query Rewriting (depending on implementation), Step-back Prompting is applied to the original query.\n",
        "    *   **Purpose:** To generate a more general or fundamental query related to the original. This helps retrieve broader context and background information that might be necessary for a comprehensive answer, especially for complex topics.\n",
        "    *   **Output:** A broader, more general query.\n",
        "\n",
        "*   **Step 6: Sub-query Decomposition:** For queries identified as potentially complex (either through initial classification or analysis during previous steps), Sub-query Decomposition is performed.\n",
        "    *   **Purpose:** To break down a multifaceted query into several simpler, more manageable sub-queries. This allows the system to retrieve specific pieces of information related to different aspects of the original complex query.\n",
        "    *   **Output:** A list of simpler sub-queries.\n",
        "\n",
        "*   **Step 7: Parallel Retrieval:** The system now utilizes the outputs from the transformation steps to perform retrieval. This is where the routing decision from Step 3 becomes crucial.\n",
        "    *   **Input for Retrieval:** The rewritten query (from Step 4), the step-back query (from Step 5), and the list of sub-queries (from Step 6, if applicable). Based on the route determined in Step 3, the system might prioritize one or more of these transformed queries and direct the search to specific knowledge bases or document subsets. For instance, a negative sentiment route might prioritize retrieval using the rewritten query on troubleshooting documents, while also using the step-back query to provide basic product information.\n",
        "    *   **Purpose:** To retrieve relevant documents or passages from the knowledge base(s) using the transformed queries.\n",
        "    *   **Output:** A collection of retrieved documents or passages.\n",
        "\n",
        "*   **Step 8: Information Synthesis and Reranking:** The retrieved documents are then processed. This might involve synthesizing information from multiple documents and reranking them based on relevance, redundancy, and potentially the original query's sentiment (e.g., highlighting information that addresses negative sentiment).\n",
        "    *   **Purpose:** To consolidate information from various sources and prioritize the most relevant content for the final response.\n",
        "    *   **Output:** A refined set of retrieved information.\n",
        "\n",
        "*   **Step 9: Response Generation:** The refined retrieved information, along with the original user query and potentially the determined sentiment and route, are passed to the language model for final response generation.\n",
        "    *   **Purpose:** To generate a coherent, informative, and contextually appropriate response to the user's original query, leveraging the retrieved information and potentially tailoring the tone based on the query's sentiment.\n",
        "    *   **Output:** The final generated response to the user.\n",
        "\n",
        "**2. Logical Ordering and Integration**\n",
        "\n",
        "The proposed order is logical because it progressively refines the understanding of the user's need:\n",
        "\n",
        "*   **Sentiment Analysis First:** Understanding sentiment early allows the system to set a potential emotional context for the interaction and inform subsequent routing.\n",
        "*   **Routing Based on Initial Analysis:** Routing immediately after sentiment (and potentially a quick query type analysis) allows the system to choose the most relevant set of tools and data sources for the query, optimizing the rest of the pipeline.\n",
        "*   **Parallel Query Transformations:** Applying Query Rewriting, Step-back Prompting, and Sub-query Decomposition in parallel (or in close sequence) generates multiple perspectives on the original query, increasing the chances of retrieving comprehensive and relevant information. The routing decision might influence *which* of these transformations are prioritized or how their results are weighted.\n",
        "*   **Retrieval Leveraging Multiple Inputs:** Using the outputs of the transformations for retrieval, guided by the routing strategy, ensures that the search is both specific (rewriting), broad (step-back), and detailed (sub-queries) as needed.\n",
        "*   **Synthesis and Generation:** Finally, synthesizing the retrieved information and generating the response brings together all the insights gained from the initial query analysis and the retrieval process.\n",
        "\n",
        "**3. How Outputs are Used**\n",
        "\n",
        "*   **Sentiment:** Informs routing, can influence reranking of retrieved documents, and helps tailor the tone of the final generated response.\n",
        "*   **Query Routing:** Determines which knowledge bases are searched, which transformation techniques are emphasized, and how the retrieved information is weighted or filtered.\n",
        "*   **Rewritten Query:** Used for a precise, targeted search for highly relevant documents.\n",
        "*   **Step-back Query:** Used for a broader search to retrieve foundational or contextual information.\n",
        "*   **Sub-queries:** Each sub-query is used for targeted retrieval of information related to specific aspects of a complex original query. The results from these individual retrievals are then combined.\n",
        "\n",
        "**4. Query Routing's Influence**\n",
        "\n",
        "Query routing, potentially influenced by sentiment analysis, significantly impacts the overall retrieval strategy:\n",
        "\n",
        "*   It acts as a traffic controller, directing queries to specialized pipelines.\n",
        "*   A negative sentiment query might be routed to a 'support' pipeline that prioritizes troubleshooting guides and uses the rewritten query with a focus on problem-solving keywords.\n",
        "*   A positive sentiment query might be routed to a 'marketing/features' pipeline that prioritizes product descriptions and testimonials, potentially emphasizing retrieval using the rewritten query and downplaying the need for step-back information.\n",
        "*   A complex, neutral query might be routed to a pipeline that heavily utilizes sub-query decomposition across multiple general knowledge bases.\n",
        "\n",
        "By integrating these techniques and strategically routing the query, the RAG system can move beyond simple keyword matching to a more intelligent, context-aware, and user-sentiment-informed retrieval and generation process."
      ],
      "metadata": {
        "id": "MDeTH8V3ISXK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd580343"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Sentiment analysis can be a valuable addition to a RAG system by providing insights into the user's emotional state, which can help improve relevance, tailor responses, and enhance user experience. Specific uses include filtering/ranking results, selecting knowledge bases, adjusting parameters, and influencing reranking based on query sentiment.\n",
        "*   Query routing allows the RAG system to intelligently direct queries to the most appropriate retrieval method or data source based on criteria such as query type, domain, intent, sentiment, keywords, and complexity. This leads to improved relevance, increased efficiency, enhanced scalability, and greater flexibility.\n",
        "*   An integrated RAG workflow can incorporate Sentiment Analysis and Query Routing early in the process. Sentiment analysis helps understand the user's emotional tone, which then informs query routing. Query routing directs the subsequent steps, potentially influencing how other techniques like Query Rewriting, Step-back Prompting, and Sub-query Decomposition are applied and which data sources are prioritized for retrieval.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Implement the integrated RAG workflow by developing components for sentiment analysis and query routing and connecting them with the existing query transformation techniques and retrieval/generation modules.\n",
        "*   Evaluate the effectiveness of the integrated system by comparing its performance (relevance, response quality, efficiency) against a baseline RAG system without these techniques, using relevant metrics and user feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPctgfb4G2cS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}